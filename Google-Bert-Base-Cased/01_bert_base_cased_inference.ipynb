{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJKt4Ds8PAjTV08c2VkbqK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linux-Server/Actix/blob/main/Google-Bert-Base-Cased/01_bert_base_cased_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgYjmcEw7mnT"
      },
      "outputs": [],
      "source": [
        "## Fine tune Base model : checkpoint = \"google-bert/bert-base-cased\"\n",
        "## Current task it can perfor :  MLM(Mask lang Modellling). and Next Sentence Prediction (NSP)\n",
        "\n",
        "checkpoint = \"google-bert/bert-base-cased\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference on MLM and NSP"
      ],
      "metadata": {
        "id": "sjdlaTGOISZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Perform inference of MLM\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"fill-mask\", model=checkpoint)\n",
        "\n",
        "pipe(\"I [MASK] this place\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6EKAhCVO-ZXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform infernence on NSP (Next sentence Prediction)\n",
        "from transformers import AutoModelForNextSentencePrediction, AutoTokenizer\n",
        "\n",
        "model = AutoModelForNextSentencePrediction.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "##\n",
        "text1 = \"I love her\"\n",
        "text2 =  \"And I am gonna marry her!\"\n",
        "\n",
        "tokenize = tokenizer(text1, text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "tokenize"
      ],
      "metadata": {
        "id": "PXrzAqqsFNsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(**tokenize)\n",
        "logits = logits.logits\n",
        "logits"
      ],
      "metadata": {
        "id": "EEhE-vhyGeLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 0 - IsNext\n",
        "# 1 - NotNext\n",
        "\n",
        "torch.nn.functional.softmax(logits, dim=-1).argmax(dim=-1)"
      ],
      "metadata": {
        "id": "mvEpeh5WGy1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBFdgb6lHu4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Inference other objectives - It will fail"
      ],
      "metadata": {
        "id": "ROnZhJUwIZAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\",  model=checkpoint)\n",
        "\n"
      ],
      "metadata": {
        "id": "tPtt17OjIghv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}